{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- \n",
    "# Tutorial for Lecture 01, created by Baojian Zhou (bjzhou@fudan.edu.cn)\n",
    "# This file is opened via Jupyter-notebook. To Install it,\n",
    "# please check details in: https://jupyter.org/install\n",
    "# install Jupyter: conda install anaconda::jupyter\n",
    "\n",
    "# --- \n",
    "# Python for beginner (Python & Pycharm)\n",
    "# If you have zero knowledge about Python, no worry. Here is an one hour course: \n",
    "#    https://www.youtube.com/watch?v=kqtD5dpn9C8\n",
    "# You can continue to learn the rest after watching this short-course.\n",
    "# Here is a simple Python code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Section 2.1 Regular Expression</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in Python, there is a built in lib re, we can import them\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Slides 57\n",
    "# Task: Find woodchuck or Woodchuck : Disjunction\n",
    "test_str = \"This string contains Woodchuck and woodchuck.\"\n",
    "result=re.search(pattern=\"[wW]oodchuck\", string=test_str)\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[wW]ooodchuck\", string=test_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the word \"woodchuck\" in the following test string\n",
    "test_str = \"interesting links to woodchucks ! and lemurs!\"\n",
    "re.search(pattern=\"woodchuck\", string=test_str)\n",
    "\n",
    "# Find !, it follows the same way:\n",
    "print(re.search(pattern=\"!\", string=test_str))\n",
    "print(re.search(pattern=\"!!\", string=test_str))\n",
    "assert re.search(pattern=\"!!\", string=test_str) == None # match nothing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find any single digit in a string.\n",
    "result=re.search(pattern=r\"[0123456789]\", string=\"plenty of 7 to 5\")\n",
    "print(result)\n",
    "result=re.search(pattern=r\"[0-9]\", string=\"plenty of 7 to 5\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---Slides 58\n",
    "# Negation: If the caret ^ is the first symbol after [,\n",
    "# the resulting pattern is negated. For example, the pattern \n",
    "# [^a] matches any single character (including special characters) except a.\n",
    "\n",
    "# -- not an upper case letter\n",
    "print(re.search(pattern=r\"[^A-Z]\", string=\"Oyfn pripetchik\"))\n",
    "\n",
    "# -- neither 'S' nor 's'\n",
    "print(re.search(pattern=r\"[^Ss]\", string=\"I have no exquisite reason for't\"))\n",
    "\n",
    "# -- not a period\n",
    "print(re.search(pattern=r\"[^.]\", string=\"our resident Djinn\"))\n",
    "\n",
    "# -- either 'e' or '^'\n",
    "print(re.search(pattern=r\"[e^]\", string=\"look up ^ now\"))\n",
    "\n",
    "# -- the pattern ‘a^b’\n",
    "print(re.search(pattern=r'a\\^b', string=r'look up a^b now'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Slides 59\n",
    "# More disjuncations\n",
    "str1 = \"Woodchucks is another name for groundhog!\"\n",
    "result = re.search(pattern=\"groundhog|woodchuck\",string=str1)\n",
    "print(result)\n",
    "\n",
    "str1 = \"Find all woodchuckk Woodchuck Groundhog groundhogxxx!\"\n",
    "result = re.findall(pattern=\"[gG]roundhog|[Ww]oodchuck\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Slides 60\n",
    "# Some special chars\n",
    "\n",
    "# ?: Optional previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou?r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# *: 0 or more of previous char\n",
    "str1 = \"Find all color colour colouur colouuur colouyr\"\n",
    "result = re.findall(pattern=\"colou*r\",string=str1)\n",
    "print(result)\n",
    "\n",
    "# +: 1 or more of previous char\n",
    "str1 = \"baa baaa baaaa baaaaa\"\n",
    "result = re.findall(pattern=\"baa+\",string=str1)\n",
    "print(result)\n",
    "# .: any char\n",
    "str1 = \"begin begun begun beg3n\"\n",
    "result = re.findall(pattern=\"beg.n\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end.\"\n",
    "result = re.findall(pattern=\"\\.$\",string=str1)\n",
    "print(result)\n",
    "str1 = \"The end? The end. #t\"\n",
    "result = re.findall(pattern=\".$\",string=str1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Slides 61\n",
    "# find all \"the\" in a raw text.\n",
    "text = \"If two sequences in an alignment share a common ancestor, \\\n",
    "mismatches can be interpreted as point mutations and gaps as indels (that \\\n",
    "is, insertion or deletion mutations) introduced in one or both lineages in \\\n",
    "the time since they diverged from one another. In sequence alignments of \\\n",
    "proteins, the degree of similarity between amino acids occupying a \\\n",
    "particular position in the sequence can be interpreted as a rough \\\n",
    "measure of how conserved a particular region or sequence motif is \\\n",
    "among lineages. The absence of substitutions, or the presence of \\\n",
    "only very conservative substitutions (that is, the substitution of \\\n",
    "amino acids whose side chains have similar biochemical properties) in \\\n",
    "a particular region of the sequence, suggest [3] that this region has \\\n",
    "structural or functional importance. Although DNA and RNA nucleotide bases \\\n",
    "are more similar to each other than are amino acids, the conservation of \\\n",
    "base pairs can indicate a similar functional or structural role.\"\n",
    "matches = re.findall(\"[^a-zA-Z][tT]he[^a-zA-Z]\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nicer way is to do the following\n",
    "\n",
    "matches = re.findall(r\"\\b[tT]he\\b\", text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Implement the task shown in Slides 62\n",
    "# You may need to\n",
    "# 1. Download a Wikipedia article xml file\n",
    "# 2. Use RE to extract links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Section 2.2, 2.3, 2.4 Words and Corpus</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to download some corpus\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import indian\n",
    "from nltk.corpus import conll2007"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word types and word instances (tokens)\n",
    "\n",
    "- **Word types** are the number of distinct words in a corpus; if the set of words in the vocabulary is $V$, the number of types is the vocabulary size $|V|$. \n",
    "\n",
    "- **Word instances** are the total number $N$ of running words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.words())\n",
    "print(f\"total number of tokens in Brown corpus: {len(brown.words())}\")\n",
    "for cat in brown.categories():\n",
    "    print(f\"category {cat} has {len(brown.words(categories=cat))} tokens\")\n",
    "print(f\"It has {len(nltk.FreqDist(w.lower() for w in brown.words()))} case-insensitive types\")\n",
    "print(f\"It has {len(nltk.FreqDist(w for w in brown.words()))} case-senstive types\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_text = brown.words(categories='news')\n",
    "fdist = len(nltk.FreqDist(w.lower() for w in news_text))\n",
    "fdist_case_sensitive = len(nltk.FreqDist(w for w in news_text))\n",
    "print(f\"there are {fdist} different words in news category!\")\n",
    "print(f\"there are {fdist_case_sensitive} case sensitive words in news category!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = len(nltk.FreqDist(w.lower() for w in brown.words()))\n",
    "fdist_case_sensitive = len(nltk.FreqDist(w for w in brown.words()))\n",
    "print(f\"there are {fdist} different words among all category!\")\n",
    "print(f\"there are {fdist_case_sensitive} case sensitive words among all category!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "print(f\"all categories of brown: {brown.categories()}\")\n",
    "print(f\"all words in news: {brown.words(categories='news')}\")\n",
    "print(brown.words(fileids=['cg22']))\n",
    "print(brown.sents(categories=['news', 'editorial', 'reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"brown corpus has {len(brown.fileids())} files in total, it belongs to {len(brown.categories())} categories\")\n",
    "print(f\"first 10 file names: {brown.fileids()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_words = gutenberg.words('austen-emma.txt')\n",
    "type(emma_words)\n",
    "print(gutenberg.words('austen-emma.txt'))\n",
    "# How many tokens in the text:\n",
    "print(\"Token count:\", len(emma_words))\n",
    "\n",
    "# What is the token at index 1000?\n",
    "print(\"token at index 1000:\", emma_words[1000])\n",
    "\n",
    "# Slice from token 1400 to 1500\n",
    "print(\"slice from 1400 to 1500:\", emma_words[1400:1500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herdan’s Law or Heap's Law\n",
    "\n",
    "- $N$: the number of word instances of corpus\n",
    "- $|V|$: the number of word types\n",
    "\n",
    "The larger the corpora we look at, the more word types we find, and in fact this relationship between $|V|$ and $N$ is called **Herdan's Law** or **Heaps' Law** after its discoverers (in linguistics and information retrieval respectively). Given $k$ and $\\beta$ positive constants, and $0<\\beta<1$, it has the following form\n",
    "\n",
    "$$\n",
    "|V|=k N^\\beta.\n",
    "$$\n",
    "\n",
    "The value of $\\beta$ depends on the corpus size and the genre, but at least for the large corpora, $\\beta$ ranges from .67 to .75. Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words. Let us test it!\n",
    "Check more on [Heap\\'s Law](https://en.wikipedia.org/wiki/Heaps%27_law)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot\n",
    "import seaborn\n",
    "import numpy as np\n",
    "# Try to center figures.\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    ".output_png {\n",
    "    display: table-cell;\n",
    "    text-align: center;\n",
    "    vertical-align: middle;\n",
    "}\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_len = []\n",
    "type_len = []\n",
    "for words in [gutenberg.words(), indian.words(), conll2007.words()]:\n",
    "    token_len.append(len(words))\n",
    "    type_len.append(len(nltk.FreqDist(w.lower() for w in words)))\n",
    "    print(token_len[-1], type_len[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(token_len)\n",
    "sorted_ind = np.argsort(token_len)\n",
    "print(sorted_ind)\n",
    "sorted_N = [token_len[_] for _ in sorted_ind]\n",
    "sorted_V = [type_len[_] for _ in sorted_ind]\n",
    "print(sorted_N, sorted_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = .7\n",
    "k = 50.\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "ax.plot(sorted_V, [k*(N**beta) for N in sorted(token_len)], c='r',marker=\"D\",linewidth=3., label=\"Herdan's Law\")\n",
    "ax.plot(sorted_V, sorted_N, c='b',marker=\"H\",linewidth=3., label=\"Empirical Corpus\")\n",
    "ax.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Section 2.5 Word Tokenization</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two type of tokenizations\n",
    "\n",
    "- **Top-down tokenization**: We define a standard and implement rules to implement that kind of tokenization.\n",
    "  - word tokenization\n",
    "  - charater tokenization\n",
    "- **Bottom-up tokenization**: We use simple statistics of letter sequences to break up words into subword tokens.\n",
    "  - subword tokenization (modern LLMs use this type!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-down (rule-based) tokenization - word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use split method via the whitespace \" \"\n",
    "text = \"\"\"While the Unix command sequence just removed all the numbers and punctuation\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But, we have punctuations, icons, and many other small issues.\n",
    "text = \"\"\"Don't you love 🤗 Transformers? We sure do.\"\"\"\n",
    "print(text.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-down tokenization by using regular expression\n",
    "pattern = r'''(?x) # set flag to allow verbose regexps\n",
    "(?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A. \n",
    "| \\w+(?:-\\w+)* # words with optional internal hyphens \n",
    "| \\$?\\d+(?:\\.\\d+)?%? # currency, percentages, e.g. $12.40, 82% \n",
    "| \\.\\.\\. # ellipsis \n",
    "| [][.,;\"'?():_`-] # these are separate tokens; includes ], [\n",
    "'''\n",
    "print(f'pattern needs to match is: \\n\\n{pattern}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Don't you love 🤗 Transformers? We sure do.\"\"\"\n",
    "print(f\"tokenized words after pattern matching: \\n\\n{nltk.regexp_tokenize(text, pattern)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy works much better\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "for token in doc: \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"While the Unix command sequence just removed all the numbers and punctuation,\n",
    "for most NLP applications we’ll . But we’ll often want\n",
    "to keep the punctuation that occurs word internally, in examples like m.p.h., Ph.D.,\n",
    "AT&T, and cap’n. Special characters and numbers will need to be kept in prices\n",
    "($45.55) and dates (01/02/06); we don’t want to segment that price into separate\n",
    "tokens of “45” and “55”. And there are URLs (https://www.stanford.edu),\n",
    "Twitter hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).\n",
    "Number expressions introduce other complications as well; while commas normally\n",
    "appear at word boundaries, commas are used inside numbers in English, every\n",
    "three digits: 555,500.50. (or sometimes periods)\n",
    "where English puts commas, for example, 555 500,50.\"\"\"\n",
    "text = text.replace(\"\\n\", \" \").strip()\n",
    "print(f\"tokenized words after pattern matching: \\n\\n{nltk.regexp_tokenize(text, pattern)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy works much better\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "for token in doc: \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization is more complex in languages like written Chinese, Japanese.\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "text = '姚明进入总决赛'\n",
    "t = TreebankWordTokenizer()\n",
    "toks = t.tokenize(text)\n",
    "print(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StanfordSegmenter for Chinese \n",
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "# Note, it needs to install jar file.\n",
    "# Alternative way to tokenize Chinese words\n",
    "# install jieba via conda as: conda install conda-forge::jieba\n",
    "# Website: https://github.com/fxsjy/jieba\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '姚明进入总决赛'\n",
    "seg_list = jieba.cut(text)\n",
    "print(\", \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "text = '姚明进入总决赛'\n",
    "doc = nlp(text)\n",
    "for token in doc: \n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-down (rule-based) tokenization - character tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.zh import Chinese\n",
    "nlp_ch = Chinese()\n",
    "print(*nlp_ch(text), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Try an example of Japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding: A Bottom-up Tokenization Algorithm\n",
    "- It has been adopted from all modern LLMs including ChatGPT, GPT-series, and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all, install GPT-4's tiktoken via: conda install conda-forge::tiktoken\n",
    "import tiktoken\n",
    "# Load an encoding\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "# Use tiktoken.encoding_for_model() to automatically load the correct encoding for a given model name.\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "print(encoding.encode(\"tiktoken is great!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count tokens by counting the length of the list returned by .encode().\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"tiktoken is great!\"\n",
    "print(f'\\\"{text}\\\" has been encoded into {num_tokens_from_string(text, \"cl100k_base\")} subwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .decode() converts a list of token integers to a string.\n",
    "encode_ids = [83, 1609, 5963, 374, 2294, 0]\n",
    "print(f'the decoded string is: \\\"{encoding.decode(encode_ids)}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Chapters 5 to 8 teach the basics of 🤗 Datasets and 🤗 Tokenizers before diving into classic NLP tasks.\\\n",
    "By the end of this part, you will be able to tackle the most common NLP problems by yourself. \\\n",
    "By the end of this part, you will be ready to apply 🤗 Transformers to (almost) any machine \\\n",
    "learning problem! E=mc^2. f(x) = x^2+y^2, print('hello world!’) baojianzhou. asdasfasdgasdg\n",
    "\"\"\"\n",
    "print(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_ids = encoding.encode(text)\n",
    "print(encoding.decode(encode_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Section 2.6 Word Normalization, Lemmatization and Stemming</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization (词形还原)\n",
    "\n",
    "- Lemmatization is the task of determining that two words have the same root, despite their surface differences.\n",
    "- **Motivation**: For some NLP situations, we also want two morphologically different forms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful system might want to also return pages\n",
    "that mention woodchuck with no s.\n",
    "- **Example 1**: The words am, are, and is have the shared lemma be.\n",
    "- **Example 2**: The words dinner and dinners both have the lemma dinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "text = \"\"\"\n",
    "The Brown Corpus, a text corpus of American English that was compiled in the 1960s at Brown University, \\\n",
    "is widely used in the field of linguistics and natural language processing. It contains about 1 million \\\n",
    "words (or \"tokens\") across a diverse range of texts from 500 sources, categorized into 15 genres, such \\\n",
    "as news, editorial, and fiction, to provide a comprehensive resource for studying the English language. \\\n",
    "This corpus has been instrumental in the development and evaluation of various computational linguistics \\\n",
    "algorithms and tools.\n",
    "\"\"\"\n",
    "text = text.replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[0], type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [token.lemma_ for token in doc]\n",
    "for ori,lemma in zip(doc[:30], lemmas[:30]):\n",
    "    print(ori, lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming (词干提取): The Porter-Stemmer method\n",
    "\n",
    "Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off words final affixes. This naive version of morphological analysis is called stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy does not provide stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"\\\n",
    "This was not the map we found in Billy Bones's chest, but \\\n",
    "an accurate copy, complete in all things-names and heights \\\n",
    "and soundings-with the single exception of the red crosses \\\n",
    "and the written notes.\\\n",
    "\"\"\"   \n",
    "porter_stemmer = PorterStemmer()\n",
    "words = word_tokenize(text)\n",
    "for word in words:\n",
    "    print(word, porter_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Section 2.7 Sentence Segmentation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Method 1: use nltk package\n",
    "# Install nltk\n",
    "import nltk\n",
    "# Download the required models\n",
    "nltk.download('punkt')  \n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"In the first part of the book we introduce the fundamental suite of algorithmic \\\n",
    "tools that make up the modern neural language model that is the heart of end-to-end \\\n",
    "NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms \\\n",
    "like computing edit distance, and then proceed to the tasks of classification, \\\n",
    "logistic regression, neural networks, proceeding through feedforward networks, recurrent \\\n",
    "networks, and then transformers. We’ll also see the role of embeddings as a \\\n",
    "model of word meaning.\"\n",
    "sentences = sent_tokenize(text)\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---\n",
    "# Method 2: A modern and fast NLP library that includes support for sentence segmentation. \n",
    "# spaCy uses a statistical model to predict sentence boundaries, which can be more accurate \n",
    "# than rule-based approaches for complex texts.\n",
    "# Install via conda: conda install conda-forge::spacy\n",
    "# Install via pip:   pip install -U spacy\n",
    "# Download data: python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Here is a sentence. Here is another one! And the last one.\")\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to install it via: python -m spacy download zh_core_web_sm\n",
    "from spacy.lang.zh.examples import sentences \n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(sentences[0])\n",
    "text = \"\"\"\\\n",
    "时光荏苒，自 2003 年我师从吴立德教授，开启自然语言处理学习与研究之路，转眼已近二十\\\n",
    "载春秋。回想当年第一次听到自然语言处理的目标 ──“让机器理解人类语言”时的兴奋，第一次\\\n",
    "看到《大规模中文文本处理》教材时的茫然，仿佛黄萱菁教授对我研究生入学的电话面试就在昨\\\n",
    "天，每周与吴老师固定交流前的紧张感依然清晰。从求学到任教，深刻感受到自然语言处理的快\\\n",
    "速发展，从基于特征的统计机器学习方法到深度神经网络模型，再到大规模预训练方法，自然语\\\n",
    "言处理研究范式的更新迭代速度也在不断加快。在本科生和研究生的自然语言处理课程教学过程\\\n",
    "中，虽然通过不断补充国际国内的近期研究进展，将最新的理论和方法通过课件和面授的形式介\\\n",
    "绍给同学们，但是系统全面的书籍仍然是不可或缺的重要资料。于是，自 2020 年起与黄萱菁教授\\\n",
    "和桂韬研究员一起开始着手本书的准备，在经过几十次的讨论和大纲和结构反复修改后，自 2021\\\n",
    "年暑假起开始了本书的写作。2022 年本书入选复旦大学七大系列百本精品教材项目和复旦大学研\\\n",
    "究生规划系列教材项目，进一步督促我们加快进度。从规划到完成，历时近三年之久，这本拙作\\\n",
    "终于完成。\"\"\"\n",
    "doc = nlp(text)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "for ind, sent in enumerate(sentences):\n",
    "    print(f\"sentence-{ind}: {sent}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Section 2.8 Minimum Edit Distance</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define minimum edit distance algorithm via dynamic programming\n",
    "def minimum_edit_distance(source, target):\n",
    "    n = len(source)\n",
    "    m = len(target)\n",
    "    d_mat = np.zeros((n + 1, m + 1))\n",
    "    for i in range(1, n + 1):\n",
    "        d_mat[i, 0] = i\n",
    "    for j in range(1, m + 1):\n",
    "        d_mat[0, j] = j\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            sub = 0 if source[i - 1] == target[j - 1] else 2\n",
    "            del_ = d_mat[i - 1][j] + 1\n",
    "            ins_ = d_mat[i][j - 1] + 1\n",
    "            d_mat[i][j] = min(del_, ins_, d_mat[i - 1][j - 1] + sub)\n",
    "    trace, align_source, align_target = backtrack_alignment(source, target, d_mat)\n",
    "    return d_mat[n, m], trace, align_source, align_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backtrack to identify actions of all minimum edits\n",
    "def backtrack_alignment(source, target, d_mat):\n",
    "    align_source, align_target = [], []\n",
    "    i = len(source)\n",
    "    j = len(target)\n",
    "    back_trace = [[i, j]]\n",
    "    while (i, j) != (0, 0):\n",
    "        sub = 0 if source[i - 1] == target[j - 1] else 2\n",
    "        del_ = d_mat[i - 1][j]\n",
    "        ins_ = d_mat[i][j - 1]\n",
    "        # substitution operation\n",
    "        if d_mat[i][j] == d_mat[i - 1][j - 1] + sub:\n",
    "            back_trace.append([i - 1, j - 1])\n",
    "            align_source = [source[i - 1]] + align_source\n",
    "            align_target = [target[j - 1]] + align_target\n",
    "            i, j = i - 1, j - 1\n",
    "        else:\n",
    "            # deletion operation\n",
    "            if d_mat[i][j] == del_ + 1:\n",
    "                back_trace.append([i - 1, j])\n",
    "                align_source = [source[i - 1]] + align_source\n",
    "                align_target = [\"*\"] + align_target\n",
    "                i, j = i - 1, j\n",
    "            # insertion operation\n",
    "            elif d_mat[i][j] == ins_ + 1:\n",
    "                back_trace.append([i, j - 1])\n",
    "                align_source = [\"*\"] + align_source\n",
    "                align_target = [target[j - 1]] + align_target\n",
    "                i, j = i, j - 1\n",
    "    return back_trace, align_source, align_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the minimum edit distance\n",
    "def test_med(source, target):\n",
    "    med, trace, align_source, align_target = minimum_edit_distance(source, target)\n",
    "    print(f\"input source: {source} and target: {target}\")\n",
    "    print(f\"med: {med}\")\n",
    "    print(f\"trace: {trace}\")\n",
    "    print(f\"aligned source: {align_source}\")\n",
    "    print(f\"aligned target: {align_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_med(source=\"INTENTION\", target=\"EXECUTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_med(source=\"AGGCTATCACCTGACCTCCAGGCCGATGCCC\", target=\"TAGCTATCACGACCGCGGTCGATTTGCCCGAC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">Other useful tutorials</h2>\n",
    "\n",
    "- Alice Zhao's NLP with Python: https://www.youtube.com/watch?v=xvqsFTUsOmc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
